---
{"dg-publish":true,"permalink":"/digital-garden/a-local-llm/","tags":["digital-garden"],"created":"2025-08-25T07:41:50.420+01:00","updated":"2025-09-03T15:25:35.426+01:00"}
---

<aÂ href="https://anapoly.co.uk/labs">Anapoly Notebook</a> | [[digital garden/Digital Garden\|Digital Garden]]

# A local LLM

**Status:** ðŸ”¸ Seed â†’ âœ… Growing â†’ ðŸ”¸ Well-formed â†’ ðŸ”¸ Fruitful â†’ ðŸ”¸ Retired

*Transparency label: AI-heavy

---

With an AI installed locally rather than in the cloud, our data remains private and and we retain full control over our information. A cost-effective way to achieve this with reasonable performance is to install the AI on a powerful, consumer-grade computer. This needs to have a strong CPU (like AMD Ryzen 9 or Intel Core i9), a high-performance NVIDIA GPU (such as RTX 4090 or 3090), ample RAM (32â€“64 GB), and fast NVMe storage. 

A high-end laptop such as a MacBook Pro M4 Max could be used if we need portability. Alternatively, a desktop machine similar to the latest gaming computers can meet this requirement at lower cost. If using a desktop machine, we have the option of installing Linux (Ubuntu LTS) to provide a stable and efficient operating system well-suited to large language models. 

An AI running locally could be accessed over an internal network or remotely through a [[digital garden/VPN client-server 1\|VPN for secure access]] over the internet. In these cases, we would want to apply [[digital garden/Additional security for VPN client-server access to local LLM 1\|additional security measures]] as appropriate to the sensitivity of our information. 

This deep-dive conversation produced by NotebookLM provides a helpful overview of the technical issues involved in running an LLM locally.

<audio controls src="https://anapoly.co.uk/labs/media/Your_Desktop,_Your_AI_A_Guide_to_Running_LLMs_Locally.m4a"></audio>